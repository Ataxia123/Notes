## So how does this look in practice?

Probably the single most important idea in this algorithm that distinguishes it from naively taking an average score from people's votes is what I call the "polarity" values. The algorithm documentation calls them fu and fn, using f for _factor_ because these are the two terms that get multiplied with each other; the more general language is in part because of a desire to eventually make fu and fn multi-dimensional.

Polarity is assigned to both users and notes. The link between _user_ IDs and the underlying Twitter accounts is intentionally kept hidden, but notes are public. In practice, the polarities generated by the algorithm, at least for the English-language data set, map very closely to the left vs right political spectrum.

Here are some examples of notes that have gotten polarities around -0.8:

  

|Note|Polarity|
|---|---|
|Anti-trans rhetoric has been amplified by some conservative Colorado lawmakers, including U.S. Rep.Â Lauren Boebert, who narrowly won re-election in Colorado's GOP-leaning 3rd Congressional District, which does not include Colorado Springs. [https://coloradosun.com/2022/11/20/colorado-springs-club-q-lgbtq-trans/](https://coloradosun.com/2022/11/20/colorado-springs-club-q-lgbtq-trans/)|-0.800|
|President Trump explicitly undermined American faith in election results in the months leading up to the 2020 election. [https://www.npr.org/2021/02/08/965342252/timeline-what-trump-told-supporters-for-months-before-they-attacked](https://www.npr.org/2021/02/08/965342252/timeline-what-trump-told-supporters-for-months-before-they-attacked) Enforcing Twitter's Terms of Service is not election interference.|-0.825|
|The 2020 election was conducted in a free and fair manner. [https://www.npr.org/2021/12/23/1065277246/trump-big-lie-jan-6-election](https://www.npr.org/2021/12/23/1065277246/trump-big-lie-jan-6-election)|-0.818|

  

Note that I am not cherry-picking here; these are literally the first three rows in the `scored_notes.tsv` spreadsheet generated by the algorithm when I ran it locally that have a polarity score (called `coreNoteFactor1` in the spreadsheet) of less than -0.8.

Now, here are some notes that have gotten polarities around +0.8. It turns out that many of these are either people talking about Brazilian politics in Portuguese or Tesla fans angrily refuting criticism of Tesla, so let me cherry-pick a bit to find a few that are not:

  

|Note|Polarity|
|---|---|
|As of 2021 data, 64% of "Black or African American" children lived in single-parent families. [https://datacenter.aecf.org/data/tables/107-children-in-single-parent-families-by-race-and-ethnicity](https://datacenter.aecf.org/data/tables/107-children-in-single-parent-families-by-race-and-ethnicity)|+0.809|
|Contrary to Rolling Stones push to claim child trafficking is "a Qanon adjacent conspiracy," child trafficking is a real and huge issue that this movie accurately depicts. Operation Underground Railroad works with multinational agencies to combat this issue. [https://ourrescue.org/](https://ourrescue.org/)|+0.840|
|Example pages from these LGBTQ+ children's books being banned can be seen here: [https://i.imgur.com/8SY6cEx.png](https://i.imgur.com/8SY6cEx.png) These books are obscene, which is not protected by the US constitution as free speech. [https://www.justice.gov/criminal-ceos/obscenity](https://www.justice.gov/criminal-ceos/obscenity) "Federal law strictly prohibits the distribution of obscene matter to minors.|+0.806|

  

Once again, it is worth reminding ourselves that the "left vs right divide" was _not_ in any way hardcoded into the algorithm; it was discovered emergently by the calculation. This suggests that if you apply this algorithm in other cultural contexts, it could automatically detect what their primary political divides are, and bridge across those too.

Meanwhile, notes that get the highest _helpfulness_ look like this. This time, because these notes are actually shown on Twitter, I can just screenshot one directly:

  

![](https://vitalik.ca/general/2023/08/16/communitynotes.html../../../../images/communitynotes/helpfulnote1.png)

  

And another one:

  

![](https://vitalik.ca/general/2023/08/16/communitynotes.html../../../../images/communitynotes/helpfulnote2.png)

  

The second one touches on highly partisan political themes more directly, but it's a clear, high-quality and informative note, and so it gets rated highly. So all in all, the algorithm seems to work, and the ability to verify the outputs of the algorithm by running the code seems to work.
